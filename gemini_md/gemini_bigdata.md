알겠습니다. 이번에는 **2~10번이 빠지지 않도록**,
**1~10 전체 가이드라인을 하나의 마크다운 블록으로 완전하게** 다시 정리합니다.
아래 **전체를 한 번에 복사해서 `.md` 파일로 저장**하시면 됩니다.

---

````markdown
# 대용량 CSV 리뷰 데이터 분석 가이드라인
## (스트리밍 로딩 → 정리 → Parquet 저장 → 분석)

본 문서는 1GB 이상 대용량 리뷰 CSV 파일을  
**메모리 오류 없이, 빠르고 안정적으로 분석**하기 위한 파이썬 기반 표준 가이드라인이다.

---

## 1️⃣ 기본 전제

- 원본 데이터 파일명: **분석 파일.csv**
- 데이터 특성
  - 리뷰 텍스트 포함
  - 결측치 및 깨진 행 존재 가능
  - 파일 크기 1GB 이상
- 분석 목표
  - CSV를 한 번만 읽는다
  - 분석 친화적인 Parquet 포맷으로 변환한다
  - 이후 분석은 빠르고 반복 가능하게 수행한다

---

## 2️⃣ 환경 준비

### 2.1 필수 라이브러리
```bash
pip install duckdb polars
````

### 2.2 권장 환경

* Python 3.9 이상
* 로컬 디스크 여유 공간 (CSV 크기 × 2 이상 권장)

---

## 3️⃣ 스트리밍 로딩 전략

### 3.1 기본 원칙

* CSV 전체를 메모리에 로딩하지 않는다
* 디스크 기반 스트리밍 방식으로 처리한다
* 분석에 필요한 컬럼만 선택한다

### 3.2 DuckDB를 사용하는 이유

* 대용량 CSV를 디스크에서 직접 스캔
* 깨진 행을 자동으로 무시 가능
* SQL 기반 집계/필터링 속도가 매우 빠름

---

## 4️⃣ 데이터 정리(정규화) 기준

### 4.1 최소 정리 원칙

* 분석에 필요한 컬럼만 유지
* 데이터 타입을 명시적으로 변환
* 결측치 및 이상값 제거
* 분석 목적에 불필요한 행 제거

### 4.2 권장 정리 규칙 예시

* `rating`: 1~5 범위만 유지
* `review_text`: NULL 또는 빈 값 제거
* 날짜 컬럼: DATE 타입으로 변환
* 중복 리뷰 제거 (가능한 경우)

---

## 5️⃣ Parquet 저장 전략 (핵심 단계)

### 5.1 Parquet를 사용하는 이유

* CSV 대비 높은 압축률
* 컬럼 단위 로딩으로 분석 속도 대폭 향상
* 반복 분석에 최적화된 포맷

### 5.2 저장 원칙

* CSV는 최초 1회만 사용
* 정리된 결과만 Parquet로 저장
* 이후 분석은 Parquet 파일만 사용

---

## 6️⃣ Parquet 기반 분석 전략

### 6.1 기본 분석 항목

* 전체 리뷰 수
* 평점 분포
* 날짜별 리뷰 추이
* 상품별 리뷰 수 및 평균 평점

### 6.2 분석 방식

* SQL 기반 GROUP BY / WHERE 중심
* 필요한 컬럼만 선택하여 처리

---

## 7️⃣ 텍스트 분석을 위한 샘플링 전략

### 7.1 전체 NLP를 피해야 하는 이유

* 연산 비용 과다
* 처리 시간 증가
* 대부분의 인사이트는 샘플로 충분

### 7.2 권장 샘플링 방식

* 평점별 균등 샘플링
* 분석 목적(부정 리뷰, 특정 키워드 등)에 따른 필터링 샘플링

---

## 8️⃣ 텍스트 전처리 가이드

### 8.1 최소 전처리 원칙

* HTML 태그 제거
* 반복 문자 축약 (ㅋㅋㅋ, ㅎㅎㅎ 등)
* 불필요한 공백 제거
* 대소문자 통일

### 8.2 전처리 목적

* 노이즈 감소
* 키워드/토픽 분석 정확도 향상

---

## 9️⃣ 키워드 및 인사이트 분석

### 9.1 빠르게 확인할 수 있는 인사이트

* 평점별 자주 등장하는 단어
* 부정(1~2점) 리뷰 주요 키워드
* 특정 키워드 포함 리뷰의 평점 분포

### 9.2 활용 예

* 불만 원인 파악
* 제품 개선 포인트 도출
* 고객 만족 요소 분석

---

## 🔟 전체 분석 흐름 요약

```
분석 파일.csv
   ↓ (DuckDB 스트리밍 로딩)
데이터 정리
   ↓
Parquet 저장
   ↓
Parquet 기반 통계 분석
   ↓
샘플링
   ↓
텍스트 분석 / 인사이트 도출
```

---

## ✅ 실무 최적화 체크리스트

* [ ] CSV 전체 로딩 금지
* [ ] 필요한 컬럼만 사용
* [ ] 데이터 정리 후 Parquet 저장
* [ ] NLP는 샘플링 후 수행
* [ ] Pandas 사용 최소화

---

## 끝

이 가이드라인을 따르면
대용량 CSV 리뷰 데이터 분석을
안정적이고 반복 가능하게 수행할 수 있다.

```

---

이번 블록은 **1️⃣부터 🔟까지 전부 포함**되어 있습니다.  
이제 그대로 복사해서 `.md` 파일로 저장하시면 됩니다.

다음으로 원하시면:
- 이 가이드 기반 **회사 표준 문서 버전**
- **초보자용 / 실무자용 분리 버전**
- **이 md에 대응하는 실행용 `.py` 파이프라인**

중 원하는 방향으로 바로 정리해드릴게요.
```
